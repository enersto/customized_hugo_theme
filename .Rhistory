<<<<<<< HEAD
<<<<<<< HEAD
M -> M_E;
M_E -> Finish;
D_E -- D_Es;
P -- Ps;
D_S -- D_Ss;
M -- Ms;
M_E -- M_Es;
Finish -- Finishs
}
')
DiagrammeR::mermaid('
graph LR
D_E[Data Exploration/Visualization]
P[Preprocessing]
D_S[Data Splitting]
M[Modeling]
M_E[Model Evaluation]
D_Es("featureplot()<br>dotPlot()<br>lift()<br>plotClassProbs()")
Ps("preProcess()<br>nearZeroVar()<br>findCorrelation()<br>findLinearCombos()")
D_Ss("createDataPartition()<br>createTimeSlices()")
Ms("train()<br>trainControl()")
M_Es("confusionMatrix()<br>resample()<br>postResample()")
Finishs(fit the final model with<br> optimal parameter set)
D_E --> D_S
D_S --> P
P --> M
M --tuning or parameters <br> set changing--> M_E
M_E -- "if largest performance value <br>or smallest mean squared error"--> Finish
M_E -- "if not enough good"--> Retune
Retune -->M
D_E --- D_Es
P --- Ps
D_S --- D_Ss
M --- Ms
M_E --- M_Es
Finish --- Finishs
classDef default fill:#ffffe6,stroke:#333,stroke-width:2px
style D_E stroke-width:4px
style P stroke-width:4px,stroke-dasharray: 5, 5
style D_S stroke-width:4px
style M stroke-width:4px
style M_E stroke-width:4px
style Finish stroke-width:4px
', height =500, width =950
)
DiagrammeR::mermaid('
graph LR
D_E[Data Exploration/Visualization]
P[Preprocessing]
D_S[Data Splitting]
M[Modeling]
M_E[Model Evaluation]
Retune[Change tune set <br or turn parameter]
D_Es("featureplot()<br>dotPlot()<br>lift()<br>plotClassProbs()")
Ps("preProcess()<br>nearZeroVar()<br>findCorrelation()<br>findLinearCombos()")
D_Ss("createDataPartition()<br>createTimeSlices()")
Ms("train()<br>trainControl()")
M_Es("confusionMatrix()<br>resample()<br>postResample()")
Finishs(fit the final model with<br> optimal parameter set)
D_E --> D_S
D_S --> P
P --> M
M --tuning or parameters <br> set changing--> M_E
M_E -- "if largest performance value <br>or smallest mean squared error"--> Finish
M_E -- "if not enough good"--> Retune
Retune -->M
D_E --- D_Es
P --- Ps
D_S --- D_Ss
M --- Ms
M_E --- M_Es
Finish --- Finishs
classDef default fill:#ffffe6,stroke:#333,stroke-width:2px
style D_E stroke-width:4px
style P stroke-width:4px,stroke-dasharray: 5, 5
style D_S stroke-width:4px
style M stroke-width:4px
style M_E stroke-width:4px
style Finish stroke-width:4px
style Finish stroke-width:4px
', height =500, width =950
)
DiagrammeR::mermaid('
graph LR
D_E[Data Exploration/Visualization]
P[Preprocessing]
D_S[Data Splitting]
M[Modeling]
M_E[Model Evaluation]
Retune[Change tune set <br or turn parameter]
D_Es("featureplot()<br>dotPlot()<br>lift()<br>plotClassProbs()")
Ps("preProcess()<br>nearZeroVar()<br>findCorrelation()<br>findLinearCombos()")
D_Ss("createDataPartition()<br>createTimeSlices()")
Ms("train()<br>trainControl()")
M_Es("confusionMatrix()<br>resample()<br>postResample()")
Finishs(fit the final model with<br> optimal parameter set)
D_E --> D_S
D_S --> P
P --> M
M --tuning or parameters <br> set changing--> M_E
M_E -- "if largest performance value <br>or smallest mean squared error"--> Finish
M_E -- "if not enough good"--> Retune
Retune -->M
D_E --- D_Es
P --- Ps
D_S --- D_Ss
M --- Ms
M_E --- M_Es
Finish --- Finishs
classDef default fill:#ffffe6,stroke:#333,stroke-width:2px
style D_E stroke-width:4px
style P stroke-width:4px,stroke-dasharray: 5, 5
style D_S stroke-width:4px
style M stroke-width:4px
style M_E stroke-width:4px
style Finish stroke-width:4px
style Retune stroke-width:4px
', height =500, width =950
)
DiagrammeR::mermaid('
graph LR
D_E[Data Exploration/Visualization]
P[Preprocessing]
D_S[Data Splitting]
M[Modeling]
M_E[Model Evaluation]
Retune["Change tune set <br or turn parameter"]
D_Es("featureplot()<br>dotPlot()<br>lift()<br>plotClassProbs()")
Ps("preProcess()<br>nearZeroVar()<br>findCorrelation()<br>findLinearCombos()")
D_Ss("createDataPartition()<br>createTimeSlices()")
Ms("train()<br>trainControl()")
M_Es("confusionMatrix()<br>resample()<br>postResample()")
Finishs(fit the final model with<br> optimal parameter set)
D_E --> D_S
D_S --> P
P --> M
M --tuning or parameters <br> set changing--> M_E
M_E -- "if largest performance value <br>or smallest mean squared error"--> Finish
M_E -- "if not enough good"--> Retune
Retune -->M
D_E --- D_Es
P --- Ps
D_S --- D_Ss
M --- Ms
M_E --- M_Es
Finish --- Finishs
classDef default fill:#ffffe6,stroke:#333,stroke-width:2px
style D_E stroke-width:4px
style P stroke-width:4px,stroke-dasharray: 5, 5
style D_S stroke-width:4px
style M stroke-width:4px
style M_E stroke-width:4px
style Finish stroke-width:4px
style Retune stroke-width:4px
', height =500, width =950
)
DiagrammeR::mermaid('
graph LR
D_E[Data Exploration/Visualization]
P[Preprocessing]
D_S[Data Splitting]
M[Modeling]
M_E[Model Evaluation]
Retune[Turn Parameter]
D_Es("featureplot()<br>dotPlot()<br>lift()<br>plotClassProbs()")
Ps("preProcess()<br>nearZeroVar()<br>findCorrelation()<br>findLinearCombos()")
D_Ss("createDataPartition()<br>createTimeSlices()")
Ms("train()<br>trainControl()")
M_Es("confusionMatrix()<br>resample()<br>postResample()")
Finishs(fit the final model with<br> optimal parameter set)
D_E --> D_S
D_S --> P
P --> M
M --tuning or parameters <br> set changing--> M_E
M_E -- "if largest performance value <br>or smallest mean squared error"--> Finish
M_E -- "if not enough good"--> Retune
Retune --> M
D_E --- D_Es
P --- Ps
D_S --- D_Ss
M --- Ms
M_E --- M_Es
Finish --- Finishs
classDef default fill:#ffffe6,stroke:#333,stroke-width:2px
style D_E stroke-width:4px
style P stroke-width:4px,stroke-dasharray: 5, 5
style D_S stroke-width:4px
style M stroke-width:4px
style M_E stroke-width:4px
style Finish stroke-width:4px
style Retune stroke-width:4px
', height =500, width =950
)
set.seed(127)
inTraining <- createDataPartition(Zoo$type, p = .75, list = FALSE)
training <- Zoo[ inTraining,]
testing  <- Zoo[-inTraining,]
View(training)
set.seed(127)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
mtry <- sqrt(ncol(x))
set.seed(127)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
mtry <- sqrt(ncol(training))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(type~., data=training, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_default)
sqrt(ncol(training))
expand.grid(.mtry=mtry)
floor(sqrt(ncol(training)))
set.seed(825)
gbmFit1 <- train(type ~ ., data = training,
method = "gbm",
trControl = fitControl,
## This last option is actually one
## for gbm() that passes through
verbose = FALSE)
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
set.seed(825)
gbmFit1 <- train(type ~ ., data = training,
method = "gbm",
trControl = fitControl,
## This last option is actually one
## for gbm() that passes through
verbose = FALSE)
gbmFit1
set.seed(127)
fitControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
mtry <- floor(sqrt(ncol(training)))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(type~., data=training, method="rf", metric=metric, tuneGrid=tunegrid, trControl=fitControl)
print(rf_default)
ggplot(rf_default)
rfClasses <- predict(rf_default, newdata = testing)
rfType <- predict(rf_default, newdata = testing)
confusionMatrix(data = rfType, testing$type)
rfType
rfType <- predict(rf_default, newdata = testing)
confusionMatrix(data = rfType, testing$type)
View(testing)
rf_default
data(oil)
inTrain <- sample(seq(along = fattyAcids), length(fattyAcids)*0.7)
training <- fattyAcids[inTrain,]
str(training)
preProcValues <- preProcess(training, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, training)
str(trainTransformed)
names(fattyAcids)
View(fattyAcids)
data(oil)
oilType
fattyAcids <- merge(fattyAcids,oilType)
View(fattyAcids)
unique(fattyAcids$y)
data(oil)
fattyAcids <- merge(fattyAcids,oilType)
fattyAcids <- setDT(fattyAcids)[,y:=
recode(y,c("A", "B","C","D", "E", "F", "G"),
c("pumpkin","sunflower","peanut",
"olive","soybean","rapeseed","corn") )]
data(oil)
fattyAcids <- merge(fattyAcids,oilType)
fattyAcids <- setDT(fattyAcids)[,y:=
dplyr::recode(y,c("A", "B","C","D", "E", "F", "G"),
c("pumpkin","sunflower","peanut",
"olive","soybean","rapeseed","corn") )]
data(oil)
fattyAcids <- merge(fattyAcids,oilType)
fattyAcids <- setDT(fattyAcids)[,y:=
plyr::mapvalue(y,c("A", "B","C","D", "E", "F", "G"),
c("pumpkin","sunflower","peanut",
"olive","soybean","rapeseed","corn") )]
data(oil)
fattyAcids <- merge(fattyAcids,oilType)
fattyAcids <- setDT(fattyAcids)[,y:=
plyr::mapvalues(y,c("A", "B","C","D", "E", "F", "G"),
c("pumpkin","sunflower","peanut",
"olive","soybean","rapeseed","corn") )]
unique(fattyAcids$y)
set.seed(127)
inTraining <- createDataPartition(fattyAcids$y, p = .75, list = FALSE)
training <- fattyAcids[ inTraining,]
testing  <- fattyAcids[-inTraining,]
bootControl <- trainControl(number = 200)
set.seed(127)
svmFit <- train(y~., training,
method = "svmRadial",
tuneLength = 5,
preProc = c("center", "scale"),
trControl = bootControl,
scaled = FALSE)
data(oil)
fattyAcids <- merge(fattyAcids,oilType)
fattyAcids <- setDT(fattyAcids)[,y:=
plyr::mapvalues(y,c("A", "B","C","D", "E", "F", "G"),
c("pumpkin","sunflower","peanut",
"olive","soybean","rapeseed","corn") )]
set.seed(127)
inTraining <- createDataPartition(fattyAcids$y, p = .75, list = FALSE)
training <- fattyAcids[ inTraining,]
testing  <- fattyAcids[-inTraining,]
preProcValues <- preProcess(training, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, training)
bootControl <- trainControl(number = 100)
set.seed(127)
svmFit <- train(y~., trainTransformed,
method = "svmRadial",
tuneLength = 5,
preProc = c("center", "scale"),
trControl = bootControl,
scaled = FALSE)
data(oil)
fattyAcids <- merge(fattyAcids,oilType)
fattyAcids <- setDT(fattyAcids)[,y:=
plyr::mapvalues(y,c("A", "B","C","D", "E", "F", "G"),
c("pumpkin","sunflower","peanut",
"olive","soybean","rapeseed","corn") )]
set.seed(127)
inTraining <- createDataPartition(fattyAcids$y, p = .75, list = FALSE)
training <- fattyAcids[ inTraining,]
testing  <- fattyAcids[-inTraining,]
preProcValues <- preProcess(training, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, training)
bootControl <- trainControl(number = 100)
set.seed(127)
svmFit <- train(y~., trainTransformed,
method = "svmRadial",
tuneLength = 5,
trControl = bootControl,
scaled = FALSE)
length(oilType)
Desc(fattyAcids$y)
View(fattyAcids)
data(oil)
fattyAcids <- merge(fattyAcids,oilType)
oilType
data(oil)
fattyAcids <- cbind(fattyAcids,oilType)
View(fattyAcids)
fattyAcids <- setDT(fattyAcids)[,y:=
plyr::mapvalues(y,c("A", "B","C","D", "E", "F", "G"),
c("pumpkin","sunflower","peanut",
"olive","soybean","rapeseed","corn") )]
View(fattyAcids)
data(oil)
fattyAcids <- cbind(fattyAcids,oilType)
fattyAcids <- setDT(fattyAcids)[,oilType:=
plyr::mapvalues(oilType,c("A", "B","C","D", "E", "F", "G"),
c("pumpkin","sunflower","peanut",
"olive","soybean","rapeseed","corn") )]
Desc(fattyAcids$oilType)
set.seed(127)
inTraining <- createDataPartition(fattyAcids$oilType, p = .75, list = FALSE)
training <- fattyAcids[ inTraining,]
testing  <- fattyAcids[-inTraining,]
Desc(training$oilType)
bootControl <- trainControl(number = 100)
set.seed(127)
svmFit <- train(oilType~., training,
method = "svmRadial",
tuneLength = 5,
preProc = c("center", "scale"),
trControl = bootControl,
scaled = FALSE)
svmFit
svmFit$finalModel
svmType <- predict(svmFit, newdata = testing)
confusionMatrix(data = svmType, testing$oilType)
svmType
testing$oilType
gbmGrid <- expand.grid(.interaction.depth = (1:5) * 2,
.n.trees = (1:10)*25, .shrinkage = .1)
gbmFit <- train(oilType ~., training,
method = "gbm", trControl = bootControl, verbose = FALSE,preProc = c("center", "scale"),
bag.fraction = 0.5, tuneGrid = gbmGrid)
varImp(svmFit$finalModel)
varImp(svmFit)
svmImp <- varImp(svmFit, scale = FALSE)
plot(simImp)
svmImp <- varImp(svmFit, scale = FALSE)
plot(simImp)
plot(svmImp)
blogdown:::serve_site()
blogdown:::serve_site()
servr::daemon_stop("1587417567384")
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
servr::daemon_stop("140518633161048")
blogdown:::insert_image_addin()
blogdown:::insert_image_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
dt <- fread("Desktop/Neuer Arbeit/CDNOW_master.txt")
library(data.table)
dt <- fread("Desktop/Neuer Arbeit/CDNOW_master.txt")
dt <- fread("Desktop/NeuerArbeit/CDNOW_master.txt")
dt <- fread("~/Desktop/NeuerArbeit/CDNOW_master.txt")
library(data.table)
dt <- fread("~/Desktop/NeuerArbeit/CDNOW_master.txt")
save.image("~/Documents/GitHub/customized_hugo_theme/Untitled.RData")
View(dt)
setnames(dt,c("id","date","numberOfBuying","account"))
View(dt)
library(anytime)
dt <-  dt[,date := anydate(date)]
View(dt)
dt <- fread("~/Desktop/NeuerArbeit/CDNOW_master.txt")
setnames(dt,c("id","date","numberOfBuying","account"))
View(dt)
dt <-  dt[,dates := anydate(date)]
View(dt)
library(data.table)
library(anytime)
dt <- fread("~/Desktop/NeuerArbeit/CDNOW_master.txt")
setnames(dt,c("id","date","numberOfBuying","account"))
dt <-  dt[,date := anydate(date)]
library(kableExtra)
library(data.table)
library(anytime)
library(kableExtra)
dt <- fread("~/Desktop/NeuerArbeit/CDNOW_master.txt")
setnames(dt,c("id","date","numberOfBuying","account"))
dt <-  dt[,date := anydate(date)]
kable(head(dt))
library(data.table)
library(anytime)
library(kableExtra)
dt <- fread("~/Desktop/NeuerArbeit/CDNOW_master.txt")
setnames(dt,c("id","date","numberOfBuying","account"))
dt <-  dt[,date := anydate(date)]
kable(sample(dt,6))
library(data.table)
library(anytime)
library(kableExtra)
dt <- fread("~/Desktop/NeuerArbeit/CDNOW_master.txt")
setnames(dt,c("id","date","numberOfBuying","account"))
dt <-  dt[,date := anydate(date)]
kable(dt[sample(.N,6)])
Desc(dt$date)
library(DescTools)
Desc(dt$date)
knitr::opts_chunk$set(
echo = TRUE,
message = FALSE,
warning =FALSE
)
options(scipen=8)
library(data.table)
library(anytime)
library(kableExtra)
dt <- fread("~/Desktop/NeuerArbeit/CDNOW_master.txt")
setnames(dt,c("id","date","numberOfBuying","account"))
dt <-  dt[,date := anydate(date)]
kable(dt[sample(.N,6)])
Desc(dt$date)
temp <- Desc(dt$date)
View(temp)
temp[[1]][["x"]]
temp
temp[[1]][["xname"]]
View(temp[[1]][["mperctab"]])
temp[[1]]
summary(temp)
str(temp)
temp
temp[[2]]
temp[[1]]
str(temp)
temp[[1]][["dperctab"]]
View(temp[[1]][["mperctab"]])
View(temp[[1]][["freq"]])
line(temp[[1]][["freq"]])
plot(temp[[1]][["freq"]])
Desc(dt$date)[[1]][["freq"]]
Desc(dt$date)[[1]][["freq"]][,1:2]
plot(Desc(dt$date)[[1]][["freq"]][,1:2])
hist(Desc(dt$date)[[1]][["freq"]][,1:2])
plot(Desc(dt$date)[[1]][["freq"]][,1:2], type = "h")
library(ggplot2)
ggplot(Desc(dt$date)[[1]][["freq"]][,1:2], type = "h") + geom_bar()
ggplot(Desc(dt$date)[[1]][["freq"]][,1:2], type = "h") + geom_histogram()
ggplot(Desc(dt$date)[[1]][["freq"]][,1:2]) + geom_histogram()
ggplot(Desc(dt$date)[[1]][["freq"]][,1:2]) + geom_bar()
ggplot(Desc(dt$date)[[1]][["freq"]][,1:2]) + geom_bar(stat_bin())
Desc(dt$date)[[1]][["freq"]][,1:2]
ggplot(Desc(dt$date)[[1]][["freq"]][,1:2]) + geom_bar(aes(level,freq))
temp <- Desc(dt$date)[[1]][["freq"]][,1:2]
View(temp)
ggplot(Desc(dt$date)[[1]][["freq"]][,1:2]) + geom_bar(stat = "identity")
ggplot(Desc(dt$date)[[1]][["freq"]][,1:2]) + geom_bar(aes(level,freq),stat = "identity")
plot(Desc(dt$date)[[1]][["freq"]][,1:2])
=======
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
Desc(train_woe$y)
train_woe <- train_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
test_woe <- test_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
Desc(train_woe$y)
m_rf <- randomForest(y ~.,train_woe,ntree = 100)
result <- resamples(list(glm = m_glm,rf = m_rf))
summary(m_rf)
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
train_woe <- train_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
test_woe <- test_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
library(pROC)
modelControl <- trainControl(method = "repeatedcv", number = 5, repeats = 2,
classProbs = TRUE, summaryFunction = twoClassSummary,
seeds = nn.seeds)
# glm
set.seed(6517)
m_glm <- train(y ~ ., data=train_woe,method="glm",family=binomial(link='logit'),
trControl=modelControl, metric = "ROC")
summary(m_glm)
# rf
rfGrid <- expand.grid(mtry = seq(from = 3, to = 18, by = 3))
set.seed(6517)
m_rf <- train(train_woe[, -1], train_woe$y,
method="rf",
ntree=100,
na.action=na.omit,
tuneGrid = rfGrid,
trControl= modelControl)
summary(m_rf)
train_pred = predict(m_rf, train_woe)
train_pred
test_woe = predict(m_rf, test_woe)
roc(test_woe[,y], test_pred)
auc <- roc(test_woe[,c("y")], test_pred)
test_woe[,c("y")]
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
result <- resamples(list(glm = m_glm,rf = m_rf,gbm = m_gbm))
bwplot(result, metric="Sen")
bwplot(result, metric="Sens")
train_pred = predict(m_glm, train_woe)
test_pred = predict(m_glm, test_woe)
train_pred
test_pred
unique(test_pred)
train_pred = predict(m_glm, train_woe)
test_pred = predict(m_glm, test_woe)
unique(train_pred)
train_pred = predict(m_rf, train_woe)
test_pred = predict(m_rf, test_woe)
unique(train_pred)
Desc(train_pred)
Desc(test_pred)
m_glm <- glm(y ~.,train_woe,family = "binomial")
m_rf <- randomForest(y ~.,train_woe,ntree = 100)
train_pred = predict(m_glm, train_woe)
test_pred = predict(m_glm, test_woe)
Desc(train_pred)
train_pred = predict(m_rf, train_woe)
test_pred = predict(m_rf, test_woe)
Desc(test_pred)
test_pred
View(test_woe)
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
train_woe <- train_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
test_woe <- test_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
m_glm <- glm(y ~.,train_woe,family = "binomial")
m_rf <- randomForest(y ~.,train_woe,ntree = 100)
train_pred = predict(m_rf, train_woe)
test_pred = predict(m_rf, test_woe)
Desc(test_pred)
summary(m_glm)
scorecard()
library(data.table)
library(scorecard)
library(caret)
library(DescTools)
options(scipen = 8)
loan = fread("C:/Users/pauke/Downloads/loan.csv")
loan <- loan[,y :=
ifelse(loan_status %in% c("Charged Off", "Default",
"Does not meet the credit policy. Status:Charged Off",
"In Grace Period","Late (16-30 days)",
"Late (31-120 days)"),"bad","good")]
loan <- loan[,y := factor(y,levels = c("good","bad"))]
knitr::kable(Desc(loan$y)[[1]][["freq"]])
loan_selected <- loan[,c("loan_amnt","int_rate","dti",
"purpose","term","annual_inc","home_ownership","emp_length","revol_bal","y")]
loan_m <- var_filter(loan_selected,y = "y")
names(loan_m)
trainIndex <- createDataPartition(loan_m$y, p = .1,
list = FALSE,
times = 1)
loan_mp <- loan_m[trainIndex,]
break_adj = list(
loan_amnt=c(8500,14000,20500))
bins_adj = woebin(
loan_mp, y="y",
breaks_list=break_adj,
print_step=0)
woebin_plot(bins_adj$loan_amnt)
set.seed(6715)
trainIndex <- createDataPartition(loan_mp$y, p = .85,
list = FALSE,
times = 1)
loan_mTrain <- loan_mp[ trainIndex,]
loan_mTest  <- loan_mp[-trainIndex,]
knitr::kable(Desc(loan_mTrain$y)[[1]][["freq"]])
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
m1 = glm( y ~ ., family = "binomial", data = train_woe)
# summary(m1)
# Select a formula-based model by AIC
m_step = step(m1, direction="both", trace = FALSE)
m2 = eval(m_step$call)
summary(m2)$coefficients
summary(m1)$coefficients
summary(m2)$coefficients
summary(m2)
scorecard()
tidy(m2)
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
m_glm = glm( y ~ ., family = "binomial", data = train_woe)
predictions <- predict(m_glm, newdata = test_set, type = "response")
predictions <- predict(m_glm, test_woe, type = "response")
# Look at the predictions range
range(predictions)
loan <- loan[,y :=
ifelse(loan_status %in% c("Charged Off", "Default",
"Does not meet the credit policy. Status:Charged Off",
"In Grace Period","Late (16-30 days)",
"Late (31-120 days)"),"bad","good")]
loan <- loan[,y := factor(y,levels = c("good","bad"))]
knitr::kable(Desc(loan$y)[[1]][["freq"]])
loan_selected <- loan[,c("loan_amnt","int_rate","dti",
"purpose","term","annual_inc","home_ownership","emp_length","revol_bal","y")]
loan_m <- var_filter(loan_selected,y = "y")
names(loan_m)
Desc(loan_m$y)
options(scipen = 8)
loan = fread("C:/Users/pauke/Downloads/loan.csv")
loan <- loan[,y :=
ifelse(loan_status %in% c("Charged Off", "Default",
"Does not meet the credit policy. Status:Charged Off",
"In Grace Period","Late (16-30 days)",
"Late (31-120 days)"),"bad","good")]
loan <- loan[,y := factor(y,levels = c("bad","good"))]
knitr::kable(Desc(loan$y)[[1]][["freq"]])
loan_selected <- loan[,c("loan_amnt","int_rate","dti",
"purpose","term","annual_inc","home_ownership","emp_length","revol_bal","y")]
loan_m <- var_filter(loan_selected,y = "y")
names(loan_m)
trainIndex <- createDataPartition(loan_m$y, p = .1,
list = FALSE,
times = 1)
loan_mp <- loan_m[trainIndex,]
Desc(loan_m$y)
loan_selected <- loan[,c("loan_amnt","int_rate","dti",
"purpose","term","annual_inc","home_ownership","emp_length","revol_bal","y")]
loan_m <- var_filter(loan_selected,y = "y")
names(loan_m)
trainIndex <- createDataPartition(loan_m$y, p = .1,
list = FALSE,
times = 1)
loan_mp <- loan_m[trainIndex,]
Desc(loan_m$y)
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
m_glm = glm( y ~ ., family = "binomial", data = train_woe)
predictions <- predict(m_glm, test_woe, type = "response")
pred_cutoff_15 <- ifelse(predictions >0.15, 1,0)
table(test_woe$y,pred_cutoff_15)
log_model_logit <- glm(loan_status ~ age + emp_cat + ir_cat + loan_amnt,
family = binomial(link = logit), data = train_woe)
log_model_logit <- glm(y ~.,
family = binomial(link = logit), data = train_woe)
log_model_probit <- glm(y ~.,
family = binomial(link = probit), data = train_woe)
log_model_cloglog <-  glm(y ~.,
family = binomial(link = cloglog), data = train_woe)
# Make predictions for all models using the test set
predictions_logit <- predict(log_model_logit, newdata = test_woe, type = "response")
predictions_probit <- predict(log_model_probit, newdata = test_woe, type = "response")
predictions_cloglog <- predict(log_model_cloglog, newdata = test_woe, type = "response")
cutoff <- 0.14
class_pred_logit <- ifelse(predictions_logit > cutoff, 1, 0)
class_pred_probit <- ifelse(predictions_probit > cutoff, 1, 0)
class_pred_cloglog <- ifelse(predictions_cloglog > cutoff, 1, 0)
# Make a confusion matrix for the three models
tab_class_logit <- table(true_val,class_pred_logit)
tab_class_logit <- table(test_woe$y,class_pred_logit)
tab_class_probit <- table(test_woe$y,class_pred_probit)
tab_class_cloglog <-  table(test_woe$y,class_pred_cloglog)
acc_logit <- sum(diag(tab_class_logit)) / nrow(test_woe)
acc_probit <- sum(diag(tab_class_probit)) / nrow(test_woe)
acc_cloglog <- sum(diag(tab_class_cloglog)) / nrow(test_woe)
acc_logit
acc_probit
acc_cloglog
diag(tab_class_logit)
nrow(test_woe)
diag(tab_class_logit)
tree_undersample <- rpart(y ~ ., method = "class",
data =  train_woe)
library(rpart)
tree_undersample <- rpart(y ~ ., method = "class",
data =  train_woe)
summary(tree_undersample)
tree_prior <- rpart(y ~ ., method = "class",
data = train_woe, parms = list(prior = c(0.92, 0.07)),
control = rpart.control(cp = 0.001))
tree_prior <- rpart(y ~ ., method = "class",
data = train_woe, parms = list(prior = c(0.93, 0.07)),
control = rpart.control(cp = 0.001))
plot(tree_prior, uniform = TRUE)
plot(tree_undersample, uniform = TRUE)
names(train_woe)
tree_undersample
set.seed(345)
tree_weights <- rpart(y ~ ., method = "class",
data = train_woe, weights = case_weights,
control = rpart.control(minsplit = 5, minbucket = 2, cp = 0.001))
scorecard
set.seed(42)
model_rf_under <- caret::train(y ~ .,
data = train_woe,
method = "rf",
trControl = modelControl)
modelControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 2,
verboseIter = FALSE,
sampling = "down")
set.seed(42)
model_rf_under <- caret::train(y ~ .,
data = train_woe,
method = "rf",
trControl = modelControl)
train_woe <- train_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
test_woe <- test_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
model_rf_under <- caret::train(y ~ .,
data = train_woe,
method = "rf",
trControl = modelControl)
final_under <- data.frame(actual = test_woe$y,
predict(model_rf_under, newdata = test_woe, type = "prob"))
final_under$predict <- ifelse(final_under$bad > 0.5, "benign", "malignant")
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
View(final_under)
final_under <- data.frame(actual = test_woe$y,
predict(model_rf_under, newdata = test_woe, type = "prob"))
final_under$predict <- ifelse(final_under$bad > 0.5, "bad", "good")
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
View(final_under)
class(final_under$predict)
final_under <- final_under[,predict := factor(predict,levels = c("bad","good"))]
final_under <- setDT(final_under)[,predict := factor(predict,levels = c("bad","good"))]
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
cm_under
install.packages("ROSE")
trainIndex <- createDataPartition(loan_m$y, p = .08,
list = FALSE,
times = 1)
loan_mp <- loan_m[trainIndex,]
Desc(loan_m$y)
break_adj = list(
loan_amnt=c(8500,14000,20500))
bins_adj = woebin(
loan_mp, y="y",
breaks_list=break_adj,
print_step=0)
woebin_plot(bins_adj$loan_amnt)
set.seed(6715)
trainIndex <- createDataPartition(loan_mp$y, p = .85,
list = FALSE,
times = 1)
loan_mTrain <- loan_mp[ trainIndex,]
loan_mTest  <- loan_mp[-trainIndex,]
knitr::kable(Desc(loan_mTrain$y)[[1]][["freq"]])
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
modelControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 2,
verboseIter = FALSE,
sampling = "rose")
set.seed(42)
model_rf_under <- caret::train(y ~ .,
data = train_woe,
method = "rf",
trControl = modelControl)
train_woe <- train_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
test_woe <- test_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
modelControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 2,
verboseIter = FALSE,
sampling = "rose")
set.seed(42)
model_rf_under <- caret::train(y ~ .,
data = train_woe,
method = "rf",
trControl = modelControl)
set.seed(6517)
m_glm <- train(y ~ ., data=train_woe,method="glm",family=binomial(link='logit'),
trControl=modelControl)
final_under <- data.table(actual = test_woe$y,
predict(m_glm, newdata = test_woe, type = "prob"))
final_under <- final_under[,predict := ifelse(final_under$bad > 0.5,
"bad", "good")][,predict :=
factor(predict,levels = c("bad","good"))]
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
View(cm_under)
cm_under
modelControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 2,
verboseIter = FALSE,
sampling = "up")
set.seed(6517)
m_glm <- train(y ~ ., data=train_woe,method="glm",family=binomial(link='logit'),
trControl=modelControl)
summary(m_glm)
final_under <- data.table(actual = test_woe$y,
predict(m_glm, newdata = test_woe, type = "prob"))
final_under <- final_under[,predict := ifelse(final_under$bad > 0.5,
"bad", "good")][,predict :=
factor(predict,levels = c("bad","good"))]
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
cm_under
final_under <- data.table(actual = test_woe$y,
predict(model_rf_under, newdata = test_woe, type = "prob"))
final_under <- final_under[,predict := ifelse(final_under$bad > 0.5,
"bad", "good")][,predict :=
factor(predict,levels = c("bad","good"))]
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
cm_under
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
log_model_logit <- glm(y ~.,
family = binomial(link = logit), data = train_woe)
log_model_probit <- glm(y ~.,
family = binomial(link = probit), data = train_woe)
log_model_cloglog <-  glm(y ~.,
family = binomial(link = cloglog), data = train_woe)
# Make predictions for all models using the test set
predictions_logit <- predict(log_model_logit, newdata = test_woe, type = "response")
predictions_probit <- predict(log_model_probit, newdata = test_woe, type = "response")
predictions_cloglog <- predict(log_model_cloglog, newdata = test_woe, type = "response")
# Use a cut-off of 14% to make binary predictions-vectors
cutoff <- 0.14
class_pred_logit <- ifelse(predictions_logit > cutoff, 1, 0)
class_pred_probit <- ifelse(predictions_probit > cutoff, 1, 0)
class_pred_cloglog <- ifelse(predictions_cloglog > cutoff, 1, 0)
# Make a confusion matrix for the three models
tab_class_logit <- table(test_woe$y,class_pred_logit)
tab_class_probit <- table(test_woe$y,class_pred_probit)
tab_class_cloglog <-  table(test_woe$y,class_pred_cloglog)
# Compute the classification accuracy for all three models
acc_logit <- sum(diag(tab_class_logit)) / nrow(test_woe)
acc_probit <- sum(diag(tab_class_probit)) / nrow(test_woe)
acc_cloglog <- sum(diag(tab_class_cloglog)) / nrow(test_woe)
acc_logit
acc_probit
acc_cloglog
library(data.table)
library(scorecard)
library(caret)
library(DescTools)
options(scipen = 8)
loan = fread("C:/Users/pauke/Downloads/loan.csv")
loan <- loan[,y :=
ifelse(loan_status %in% c("Charged Off", "Default",
"Does not meet the credit policy. Status:Charged Off",
"In Grace Period","Late (16-30 days)",
"Late (31-120 days)"),"bad","good")]
loan <- loan[,y := factor(y,levels = c("bad","good"))]
knitr::kable(Desc(loan$y)[[1]][["freq"]])
loan_selected <- loan[,c("loan_amnt","int_rate","dti",
"purpose","term","annual_inc","home_ownership","emp_length","revol_bal","y")]
loan_m <- var_filter(loan_selected,y = "y")
names(loan_m)
trainIndex <- createDataPartition(loan_m$y, p = .08,
list = FALSE,
times = 1)
loan_mp <- loan_m[trainIndex,]
Desc(loan_m$y)
break_adj = list(
loan_amnt=c(8500,14000,20500))
bins_adj = woebin(
loan_mp, y="y",
breaks_list=break_adj,
print_step=0)
woebin_plot(bins_adj$loan_amnt)
set.seed(6715)
trainIndex <- createDataPartition(loan_mp$y, p = .85,
list = FALSE,
times = 1)
loan_mTrain <- loan_mp[ trainIndex,]
loan_mTest  <- loan_mp[-trainIndex,]
knitr::kable(Desc(loan_mTrain$y)[[1]][["freq"]])
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
pred_logit <- predict(mlog_logit, newdata = test_woe, type = "response")
mlog_logit <- glm(y ~.,family = binomial(link = logit), data = train_woe)
mlog_probit <- glm(y ~.,family = binomial(link = probit), data = train_woe)
mlog_cloglog <- glm(y ~.,family = binomial(link = cloglog), data = train_woe)
pred_logit <- predict(mlog_logit, newdata = test_woe, type = "response")
pred_probit <- predict(mlog_probit, newdata = test_woe, type = "response")
pred_cloglog <- predict(mlog_cloglog, newdata = test_woe, type = "response")
perf_eva(test_woe$y, pred_logit, title="test")
pred_logit
Desc(pred_cloglog)
perf_train = perf_eva(test_woe$y, pred_logit, title="test")
roc(test_woe[,y], pred_probit)
library(pROC)
roc(test_woe[,y], pred_probit)
plot(pred_cloglog)
auc <- roc(test_woe[,y], pred_cloglog)
roc(test_woe[,y], pred_cloglog)
roc_logit <- roc(test_woe[,y], pred_logit)
roc_probit <- roc(test_woe[,y], pred_probit)
roc_cloglog <- roc(test_woe[,y], pred_cloglog)
roc.test(roc_logit,roc_probit,roc_cloglog)
plot(roc_logit,roc_probit,roc_cloglog)
roc_logit
roc_probit
roc_cloglog
roc.test(roc_logit,roc_probit,roc_cloglog)
roc.test(roc_logit,roc_probit, reuse.auc=FALSE)
roc.test(roc_cloglog,roc_probit, reuse.auc=FALSE)
roc_logit
roc_probit
roc_cloglog
# Use a cut-off of 14% to make binary predictions-vectors
cutoff <- 0.14
class_pred_logit <- ifelse(pred_logit > cutoff, 1, 0)
class_pred_probit <- ifelse(pred_probit > cutoff, 1, 0)
class_pred_cloglog <- ifelse(pred_cloglog > cutoff, 1, 0)
# Make a confusion matrix for the three models
tab_class_logit <- table(test_woe$y,class_pred_logit)
tab_class_probit <- table(test_woe$y,class_pred_probit)
tab_class_cloglog <-  table(test_woe$y,class_pred_cloglog)
# Compute the classification accuracy for all three models
acc_logit <- sum(diag(tab_class_logit)) / nrow(test_woe)
acc_probit <- sum(diag(tab_class_probit)) / nrow(test_woe)
acc_cloglog <- sum(diag(tab_class_cloglog)) / nrow(test_woe)
acc_logit
acc_probit
acc_cloglog
rm(class_pred_logit,class_pred_probit,class_pred_cloglog,
tab_class_logit,tab_class_probit,tab_class_cloglog,acc_probit,acc_logit,
acc_cloglog)
card = scorecard(bins_adj, mlog_cloglog)
data.table::rbindlist(card, fill=TRUE)[c(1,37:40),1:4]
card[["annual_inc"]]
train_score = scorecard_ply(loan_mTrain, card, only_total_score=TRUE, print_step=0)
test_score = scorecard_ply(loan_mTest, card, only_total_score=TRUE, print_step=0)
View(train_score)
View(test_score)
perf_eva(loan_mTest$y, pred_cloglog, title="test")
pred_cloglog
perf_eva
perf_eva(test_woe$y, pred_cloglog, title="test")
test_woe$y
scorecard::perf_eva(test_woe$y, pred_cloglog, title="test")
pred_cloglog <- predict.train(mlog_cloglog, newdata = test_woe, type = "response")
pred_cloglog <- predict(mlog_cloglog, newdata = test_woe, type = "response")
train_score = scorecard_ply(loan_mTrain, card, only_total_score=F, print_step=0)
test_score = scorecard_ply(loan_mTest, card, only_total_score=F, print_step=0)
View(train_score)
perf_psi(
score = list(train = train_score, test = test_score),
label = list(train = loan_mTrain$y, test = loan_mTtest$y) )
perf_psi(
score = list(train = train_score, test = test_score),
label = list(train = loan_mTrain$y, test = loan_mTest$y) )
psi <- perf_psi(
score = list(train = train_score, test = test_score),
label = list(train = loan_mTrain$y, test = loan_mTest$y) )
psi$psi
knitr::kable(psi$psi)
View(psi)
psi$pic$score
blogdown:::serve_site()
run servr::daemon_stop("1932750307416")
servr::daemon_stop("1932750307416")
>>>>>>> 05e2bc2f3d649f8a7e5be221055d9ca727d00c95
=======
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
Desc(train_woe$y)
train_woe <- train_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
test_woe <- test_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
Desc(train_woe$y)
m_rf <- randomForest(y ~.,train_woe,ntree = 100)
result <- resamples(list(glm = m_glm,rf = m_rf))
summary(m_rf)
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
train_woe <- train_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
test_woe <- test_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
library(pROC)
modelControl <- trainControl(method = "repeatedcv", number = 5, repeats = 2,
classProbs = TRUE, summaryFunction = twoClassSummary,
seeds = nn.seeds)
# glm
set.seed(6517)
m_glm <- train(y ~ ., data=train_woe,method="glm",family=binomial(link='logit'),
trControl=modelControl, metric = "ROC")
summary(m_glm)
# rf
rfGrid <- expand.grid(mtry = seq(from = 3, to = 18, by = 3))
set.seed(6517)
m_rf <- train(train_woe[, -1], train_woe$y,
method="rf",
ntree=100,
na.action=na.omit,
tuneGrid = rfGrid,
trControl= modelControl)
summary(m_rf)
train_pred = predict(m_rf, train_woe)
train_pred
test_woe = predict(m_rf, test_woe)
roc(test_woe[,y], test_pred)
auc <- roc(test_woe[,c("y")], test_pred)
test_woe[,c("y")]
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
result <- resamples(list(glm = m_glm,rf = m_rf,gbm = m_gbm))
bwplot(result, metric="Sen")
bwplot(result, metric="Sens")
train_pred = predict(m_glm, train_woe)
test_pred = predict(m_glm, test_woe)
train_pred
test_pred
unique(test_pred)
train_pred = predict(m_glm, train_woe)
test_pred = predict(m_glm, test_woe)
unique(train_pred)
train_pred = predict(m_rf, train_woe)
test_pred = predict(m_rf, test_woe)
unique(train_pred)
Desc(train_pred)
Desc(test_pred)
m_glm <- glm(y ~.,train_woe,family = "binomial")
m_rf <- randomForest(y ~.,train_woe,ntree = 100)
train_pred = predict(m_glm, train_woe)
test_pred = predict(m_glm, test_woe)
Desc(train_pred)
train_pred = predict(m_rf, train_woe)
test_pred = predict(m_rf, test_woe)
Desc(test_pred)
test_pred
View(test_woe)
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
train_woe <- train_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
test_woe <- test_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
m_glm <- glm(y ~.,train_woe,family = "binomial")
m_rf <- randomForest(y ~.,train_woe,ntree = 100)
train_pred = predict(m_rf, train_woe)
test_pred = predict(m_rf, test_woe)
Desc(test_pred)
summary(m_glm)
scorecard()
library(data.table)
library(scorecard)
library(caret)
library(DescTools)
options(scipen = 8)
loan = fread("C:/Users/pauke/Downloads/loan.csv")
loan <- loan[,y :=
ifelse(loan_status %in% c("Charged Off", "Default",
"Does not meet the credit policy. Status:Charged Off",
"In Grace Period","Late (16-30 days)",
"Late (31-120 days)"),"bad","good")]
loan <- loan[,y := factor(y,levels = c("good","bad"))]
knitr::kable(Desc(loan$y)[[1]][["freq"]])
loan_selected <- loan[,c("loan_amnt","int_rate","dti",
"purpose","term","annual_inc","home_ownership","emp_length","revol_bal","y")]
loan_m <- var_filter(loan_selected,y = "y")
names(loan_m)
trainIndex <- createDataPartition(loan_m$y, p = .1,
list = FALSE,
times = 1)
loan_mp <- loan_m[trainIndex,]
break_adj = list(
loan_amnt=c(8500,14000,20500))
bins_adj = woebin(
loan_mp, y="y",
breaks_list=break_adj,
print_step=0)
woebin_plot(bins_adj$loan_amnt)
set.seed(6715)
trainIndex <- createDataPartition(loan_mp$y, p = .85,
list = FALSE,
times = 1)
loan_mTrain <- loan_mp[ trainIndex,]
loan_mTest  <- loan_mp[-trainIndex,]
knitr::kable(Desc(loan_mTrain$y)[[1]][["freq"]])
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
m1 = glm( y ~ ., family = "binomial", data = train_woe)
# summary(m1)
# Select a formula-based model by AIC
m_step = step(m1, direction="both", trace = FALSE)
m2 = eval(m_step$call)
summary(m2)$coefficients
summary(m1)$coefficients
summary(m2)$coefficients
summary(m2)
scorecard()
tidy(m2)
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
m_glm = glm( y ~ ., family = "binomial", data = train_woe)
predictions <- predict(m_glm, newdata = test_set, type = "response")
predictions <- predict(m_glm, test_woe, type = "response")
# Look at the predictions range
range(predictions)
loan <- loan[,y :=
ifelse(loan_status %in% c("Charged Off", "Default",
"Does not meet the credit policy. Status:Charged Off",
"In Grace Period","Late (16-30 days)",
"Late (31-120 days)"),"bad","good")]
loan <- loan[,y := factor(y,levels = c("good","bad"))]
knitr::kable(Desc(loan$y)[[1]][["freq"]])
loan_selected <- loan[,c("loan_amnt","int_rate","dti",
"purpose","term","annual_inc","home_ownership","emp_length","revol_bal","y")]
loan_m <- var_filter(loan_selected,y = "y")
names(loan_m)
Desc(loan_m$y)
options(scipen = 8)
loan = fread("C:/Users/pauke/Downloads/loan.csv")
loan <- loan[,y :=
ifelse(loan_status %in% c("Charged Off", "Default",
"Does not meet the credit policy. Status:Charged Off",
"In Grace Period","Late (16-30 days)",
"Late (31-120 days)"),"bad","good")]
loan <- loan[,y := factor(y,levels = c("bad","good"))]
knitr::kable(Desc(loan$y)[[1]][["freq"]])
loan_selected <- loan[,c("loan_amnt","int_rate","dti",
"purpose","term","annual_inc","home_ownership","emp_length","revol_bal","y")]
loan_m <- var_filter(loan_selected,y = "y")
names(loan_m)
trainIndex <- createDataPartition(loan_m$y, p = .1,
list = FALSE,
times = 1)
loan_mp <- loan_m[trainIndex,]
Desc(loan_m$y)
loan_selected <- loan[,c("loan_amnt","int_rate","dti",
"purpose","term","annual_inc","home_ownership","emp_length","revol_bal","y")]
loan_m <- var_filter(loan_selected,y = "y")
names(loan_m)
trainIndex <- createDataPartition(loan_m$y, p = .1,
list = FALSE,
times = 1)
loan_mp <- loan_m[trainIndex,]
Desc(loan_m$y)
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
m_glm = glm( y ~ ., family = "binomial", data = train_woe)
predictions <- predict(m_glm, test_woe, type = "response")
pred_cutoff_15 <- ifelse(predictions >0.15, 1,0)
table(test_woe$y,pred_cutoff_15)
log_model_logit <- glm(loan_status ~ age + emp_cat + ir_cat + loan_amnt,
family = binomial(link = logit), data = train_woe)
log_model_logit <- glm(y ~.,
family = binomial(link = logit), data = train_woe)
log_model_probit <- glm(y ~.,
family = binomial(link = probit), data = train_woe)
log_model_cloglog <-  glm(y ~.,
family = binomial(link = cloglog), data = train_woe)
# Make predictions for all models using the test set
predictions_logit <- predict(log_model_logit, newdata = test_woe, type = "response")
predictions_probit <- predict(log_model_probit, newdata = test_woe, type = "response")
predictions_cloglog <- predict(log_model_cloglog, newdata = test_woe, type = "response")
cutoff <- 0.14
class_pred_logit <- ifelse(predictions_logit > cutoff, 1, 0)
class_pred_probit <- ifelse(predictions_probit > cutoff, 1, 0)
class_pred_cloglog <- ifelse(predictions_cloglog > cutoff, 1, 0)
# Make a confusion matrix for the three models
tab_class_logit <- table(true_val,class_pred_logit)
tab_class_logit <- table(test_woe$y,class_pred_logit)
tab_class_probit <- table(test_woe$y,class_pred_probit)
tab_class_cloglog <-  table(test_woe$y,class_pred_cloglog)
acc_logit <- sum(diag(tab_class_logit)) / nrow(test_woe)
acc_probit <- sum(diag(tab_class_probit)) / nrow(test_woe)
acc_cloglog <- sum(diag(tab_class_cloglog)) / nrow(test_woe)
acc_logit
acc_probit
acc_cloglog
diag(tab_class_logit)
nrow(test_woe)
diag(tab_class_logit)
tree_undersample <- rpart(y ~ ., method = "class",
data =  train_woe)
library(rpart)
tree_undersample <- rpart(y ~ ., method = "class",
data =  train_woe)
summary(tree_undersample)
tree_prior <- rpart(y ~ ., method = "class",
data = train_woe, parms = list(prior = c(0.92, 0.07)),
control = rpart.control(cp = 0.001))
tree_prior <- rpart(y ~ ., method = "class",
data = train_woe, parms = list(prior = c(0.93, 0.07)),
control = rpart.control(cp = 0.001))
plot(tree_prior, uniform = TRUE)
plot(tree_undersample, uniform = TRUE)
names(train_woe)
tree_undersample
set.seed(345)
tree_weights <- rpart(y ~ ., method = "class",
data = train_woe, weights = case_weights,
control = rpart.control(minsplit = 5, minbucket = 2, cp = 0.001))
scorecard
set.seed(42)
model_rf_under <- caret::train(y ~ .,
data = train_woe,
method = "rf",
trControl = modelControl)
modelControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 2,
verboseIter = FALSE,
sampling = "down")
set.seed(42)
model_rf_under <- caret::train(y ~ .,
data = train_woe,
method = "rf",
trControl = modelControl)
train_woe <- train_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
test_woe <- test_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
model_rf_under <- caret::train(y ~ .,
data = train_woe,
method = "rf",
trControl = modelControl)
final_under <- data.frame(actual = test_woe$y,
predict(model_rf_under, newdata = test_woe, type = "prob"))
final_under$predict <- ifelse(final_under$bad > 0.5, "benign", "malignant")
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
View(final_under)
final_under <- data.frame(actual = test_woe$y,
predict(model_rf_under, newdata = test_woe, type = "prob"))
final_under$predict <- ifelse(final_under$bad > 0.5, "bad", "good")
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
View(final_under)
class(final_under$predict)
final_under <- final_under[,predict := factor(predict,levels = c("bad","good"))]
final_under <- setDT(final_under)[,predict := factor(predict,levels = c("bad","good"))]
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
cm_under
install.packages("ROSE")
trainIndex <- createDataPartition(loan_m$y, p = .08,
list = FALSE,
times = 1)
loan_mp <- loan_m[trainIndex,]
Desc(loan_m$y)
break_adj = list(
loan_amnt=c(8500,14000,20500))
bins_adj = woebin(
loan_mp, y="y",
breaks_list=break_adj,
print_step=0)
woebin_plot(bins_adj$loan_amnt)
set.seed(6715)
trainIndex <- createDataPartition(loan_mp$y, p = .85,
list = FALSE,
times = 1)
loan_mTrain <- loan_mp[ trainIndex,]
loan_mTest  <- loan_mp[-trainIndex,]
knitr::kable(Desc(loan_mTrain$y)[[1]][["freq"]])
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
modelControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 2,
verboseIter = FALSE,
sampling = "rose")
set.seed(42)
model_rf_under <- caret::train(y ~ .,
data = train_woe,
method = "rf",
trControl = modelControl)
train_woe <- train_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
test_woe <- test_woe[,y := ifelse(y ==1,"bad","good")][,y := factor(y,levels = c("good","bad"))]
modelControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 2,
verboseIter = FALSE,
sampling = "rose")
set.seed(42)
model_rf_under <- caret::train(y ~ .,
data = train_woe,
method = "rf",
trControl = modelControl)
set.seed(6517)
m_glm <- train(y ~ ., data=train_woe,method="glm",family=binomial(link='logit'),
trControl=modelControl)
final_under <- data.table(actual = test_woe$y,
predict(m_glm, newdata = test_woe, type = "prob"))
final_under <- final_under[,predict := ifelse(final_under$bad > 0.5,
"bad", "good")][,predict :=
factor(predict,levels = c("bad","good"))]
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
View(cm_under)
cm_under
modelControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 2,
verboseIter = FALSE,
sampling = "up")
set.seed(6517)
m_glm <- train(y ~ ., data=train_woe,method="glm",family=binomial(link='logit'),
trControl=modelControl)
summary(m_glm)
final_under <- data.table(actual = test_woe$y,
predict(m_glm, newdata = test_woe, type = "prob"))
final_under <- final_under[,predict := ifelse(final_under$bad > 0.5,
"bad", "good")][,predict :=
factor(predict,levels = c("bad","good"))]
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
cm_under
final_under <- data.table(actual = test_woe$y,
predict(model_rf_under, newdata = test_woe, type = "prob"))
final_under <- final_under[,predict := ifelse(final_under$bad > 0.5,
"bad", "good")][,predict :=
factor(predict,levels = c("bad","good"))]
cm_under <- confusionMatrix(final_under$predict, test_woe$y)
cm_under
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
log_model_logit <- glm(y ~.,
family = binomial(link = logit), data = train_woe)
log_model_probit <- glm(y ~.,
family = binomial(link = probit), data = train_woe)
log_model_cloglog <-  glm(y ~.,
family = binomial(link = cloglog), data = train_woe)
# Make predictions for all models using the test set
predictions_logit <- predict(log_model_logit, newdata = test_woe, type = "response")
predictions_probit <- predict(log_model_probit, newdata = test_woe, type = "response")
predictions_cloglog <- predict(log_model_cloglog, newdata = test_woe, type = "response")
# Use a cut-off of 14% to make binary predictions-vectors
cutoff <- 0.14
class_pred_logit <- ifelse(predictions_logit > cutoff, 1, 0)
class_pred_probit <- ifelse(predictions_probit > cutoff, 1, 0)
class_pred_cloglog <- ifelse(predictions_cloglog > cutoff, 1, 0)
# Make a confusion matrix for the three models
tab_class_logit <- table(test_woe$y,class_pred_logit)
tab_class_probit <- table(test_woe$y,class_pred_probit)
tab_class_cloglog <-  table(test_woe$y,class_pred_cloglog)
# Compute the classification accuracy for all three models
acc_logit <- sum(diag(tab_class_logit)) / nrow(test_woe)
acc_probit <- sum(diag(tab_class_probit)) / nrow(test_woe)
acc_cloglog <- sum(diag(tab_class_cloglog)) / nrow(test_woe)
acc_logit
acc_probit
acc_cloglog
library(data.table)
library(scorecard)
library(caret)
library(DescTools)
options(scipen = 8)
loan = fread("C:/Users/pauke/Downloads/loan.csv")
loan <- loan[,y :=
ifelse(loan_status %in% c("Charged Off", "Default",
"Does not meet the credit policy. Status:Charged Off",
"In Grace Period","Late (16-30 days)",
"Late (31-120 days)"),"bad","good")]
loan <- loan[,y := factor(y,levels = c("bad","good"))]
knitr::kable(Desc(loan$y)[[1]][["freq"]])
loan_selected <- loan[,c("loan_amnt","int_rate","dti",
"purpose","term","annual_inc","home_ownership","emp_length","revol_bal","y")]
loan_m <- var_filter(loan_selected,y = "y")
names(loan_m)
trainIndex <- createDataPartition(loan_m$y, p = .08,
list = FALSE,
times = 1)
loan_mp <- loan_m[trainIndex,]
Desc(loan_m$y)
break_adj = list(
loan_amnt=c(8500,14000,20500))
bins_adj = woebin(
loan_mp, y="y",
breaks_list=break_adj,
print_step=0)
woebin_plot(bins_adj$loan_amnt)
set.seed(6715)
trainIndex <- createDataPartition(loan_mp$y, p = .85,
list = FALSE,
times = 1)
loan_mTrain <- loan_mp[ trainIndex,]
loan_mTest  <- loan_mp[-trainIndex,]
knitr::kable(Desc(loan_mTrain$y)[[1]][["freq"]])
train_woe = woebin_ply(
loan_mTrain, bins_adj)
test_woe = woebin_ply(
loan_mTest, bins_adj)
pred_logit <- predict(mlog_logit, newdata = test_woe, type = "response")
mlog_logit <- glm(y ~.,family = binomial(link = logit), data = train_woe)
mlog_probit <- glm(y ~.,family = binomial(link = probit), data = train_woe)
mlog_cloglog <- glm(y ~.,family = binomial(link = cloglog), data = train_woe)
pred_logit <- predict(mlog_logit, newdata = test_woe, type = "response")
pred_probit <- predict(mlog_probit, newdata = test_woe, type = "response")
pred_cloglog <- predict(mlog_cloglog, newdata = test_woe, type = "response")
perf_eva(test_woe$y, pred_logit, title="test")
pred_logit
Desc(pred_cloglog)
perf_train = perf_eva(test_woe$y, pred_logit, title="test")
roc(test_woe[,y], pred_probit)
library(pROC)
roc(test_woe[,y], pred_probit)
plot(pred_cloglog)
auc <- roc(test_woe[,y], pred_cloglog)
roc(test_woe[,y], pred_cloglog)
roc_logit <- roc(test_woe[,y], pred_logit)
roc_probit <- roc(test_woe[,y], pred_probit)
roc_cloglog <- roc(test_woe[,y], pred_cloglog)
roc.test(roc_logit,roc_probit,roc_cloglog)
plot(roc_logit,roc_probit,roc_cloglog)
roc_logit
roc_probit
roc_cloglog
roc.test(roc_logit,roc_probit,roc_cloglog)
roc.test(roc_logit,roc_probit, reuse.auc=FALSE)
roc.test(roc_cloglog,roc_probit, reuse.auc=FALSE)
roc_logit
roc_probit
roc_cloglog
# Use a cut-off of 14% to make binary predictions-vectors
cutoff <- 0.14
class_pred_logit <- ifelse(pred_logit > cutoff, 1, 0)
class_pred_probit <- ifelse(pred_probit > cutoff, 1, 0)
class_pred_cloglog <- ifelse(pred_cloglog > cutoff, 1, 0)
# Make a confusion matrix for the three models
tab_class_logit <- table(test_woe$y,class_pred_logit)
tab_class_probit <- table(test_woe$y,class_pred_probit)
tab_class_cloglog <-  table(test_woe$y,class_pred_cloglog)
# Compute the classification accuracy for all three models
acc_logit <- sum(diag(tab_class_logit)) / nrow(test_woe)
acc_probit <- sum(diag(tab_class_probit)) / nrow(test_woe)
acc_cloglog <- sum(diag(tab_class_cloglog)) / nrow(test_woe)
acc_logit
acc_probit
acc_cloglog
rm(class_pred_logit,class_pred_probit,class_pred_cloglog,
tab_class_logit,tab_class_probit,tab_class_cloglog,acc_probit,acc_logit,
acc_cloglog)
card = scorecard(bins_adj, mlog_cloglog)
data.table::rbindlist(card, fill=TRUE)[c(1,37:40),1:4]
card[["annual_inc"]]
train_score = scorecard_ply(loan_mTrain, card, only_total_score=TRUE, print_step=0)
test_score = scorecard_ply(loan_mTest, card, only_total_score=TRUE, print_step=0)
View(train_score)
View(test_score)
perf_eva(loan_mTest$y, pred_cloglog, title="test")
pred_cloglog
perf_eva
perf_eva(test_woe$y, pred_cloglog, title="test")
test_woe$y
scorecard::perf_eva(test_woe$y, pred_cloglog, title="test")
pred_cloglog <- predict.train(mlog_cloglog, newdata = test_woe, type = "response")
pred_cloglog <- predict(mlog_cloglog, newdata = test_woe, type = "response")
train_score = scorecard_ply(loan_mTrain, card, only_total_score=F, print_step=0)
test_score = scorecard_ply(loan_mTest, card, only_total_score=F, print_step=0)
View(train_score)
perf_psi(
score = list(train = train_score, test = test_score),
label = list(train = loan_mTrain$y, test = loan_mTtest$y) )
perf_psi(
score = list(train = train_score, test = test_score),
label = list(train = loan_mTrain$y, test = loan_mTest$y) )
psi <- perf_psi(
score = list(train = train_score, test = test_score),
label = list(train = loan_mTrain$y, test = loan_mTest$y) )
psi$psi
knitr::kable(psi$psi)
View(psi)
psi$pic$score
blogdown:::serve_site()
run servr::daemon_stop("1932750307416")
servr::daemon_stop("1932750307416")
>>>>>>> 05e2bc2f3d649f8a7e5be221055d9ca727d00c95
