---
title: "Workflow with Caret"
author: "pauke"
date: '2019-01-31'
categories:
  - R
  - english
tags: ["caret","machine learning"]
output:
  blogdown::html_page:
    toc: true
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning =FALSE
)
```

All the packages needed in here:
```{r message=FALSE, warning=FALSE}
library(knitr)
library(caret)
library(DiagrammeR)#for the flowchart
library(DescTools)# for summary of data
library(proxy)#maxDissim
library(mlbench)#data zoo
library(data.table)
library(fastDummies) #for dummy variables

options(scipen=8)

```


## Foreword

This is a workflow about [caret](https://topepo.github.io/caret/), which is a creating predicted models collecting toolkit framework, integrating all activities related to model development from other R packages. Caret can be also treated as a tool to get started or get familiar with machine learning, especially in supervised and unsupervised learning part that is good at. 

The article is the usual thought of using caret and machine learning, more focusing on general use situation. And I'll try to offer a holistic point of machine learning, the straight using examples and the way to find more concrete field. That means I would not present the specific algorithms, tuning methods or fitted fields.

## Structure

All tools in caret can be classified in the [ducument](https://topepo.github.io/caret/) like this:

> - data splitting
> - pre-processing(depending on data set situation)
> - feature selection
> - model tuning using resampling
> - variable importance estimation and the model measuring  

Link to the crucial function in each part, all the part of caret workflow is:

```{r echo=FALSE, message=FALSE, warning=FALSE}
DiagrammeR::mermaid('
graph LR
D_E[Data Exploration/Visualization]
P[Preprocessing]
D_S[Data Splitting]
M[Modeling]
M_E[Model Evaluation]
Retune[Turn Parameter]

D_Es("featureplot()<br>dotPlot()<br>lift()<br>plotClassProbs()")
Ps("preProcess()<br>nearZeroVar()<br>findCorrelation()<br>findLinearCombos()")
D_Ss("createDataPartition()<br>createTimeSlices()")
Ms("train()<br>trainControl()")
M_Es("confusionMatrix()<br>resample()<br>postResample()")
Finishs(fit the final model with<br> optimal parameter set)


D_E --> D_S 
D_S --> P  
P --> M
M --tuning or parameters <br> set changing--> M_E
M_E -- "if largest performance value <br>or smallest mean squared error"--> Finish
M_E -- "if not enough good"--> Retune
Retune --> M


D_E --- D_Es
P --- Ps
D_S --- D_Ss
M --- Ms
M_E --- M_Es
Finish --- Finishs

classDef default fill:#ffffe6,stroke:#333,stroke-width:2px

style D_E stroke-width:4px
style P stroke-width:4px,stroke-dasharray: 5, 5
style D_S stroke-width:4px
style M stroke-width:4px
style M_E stroke-width:4px
style Finish stroke-width:4px
style Retune stroke-width:4px

', height =500, width =950
)

```


By the way,as I said in foreword, this post would like more pay attention on the process of caret using. The part of data exploration is deeply dependent on the familiarity about the data you get to deal with, and can be replaced by another professional packages. If you are more interesting on the visualization tools of caret, just jump to this [chapter](http://topepo.github.io/caret/visualizations.html).

## Data Splitting

More specific matter of data splitting, you can find on the [document](http://topepo.github.io/caret/data-splitting.html) and the this article [Preprocessign Data with caret](http://rismyhammer.com/ml/Pre-Processing.html)

situation | splitting mathods
----------|------------------
supervised learning|`createDataPartition()`
unsupervised learning | `maxDissim()`
time serias | `trainControl(method = "timeslice"...)`


### Based on Outcome

`createDataPartition()` is the splitting function that data set has clear dependent variables, which means it's fitted for supervised learning. `createDataPartition()`is the one of the most useful feature function of caret package, which makes data splitting to be more orderly and approachable.

This method can generate balant data split. When the dependent variables are factor variables, the splitting will based on the distribution of factors.

```{r echo=TRUE, warning=FALSE}

data(scat)
#set seed and create splitting index
set.seed(3456)
trainIndex <- createDataPartition(scat$Species, p = .8, 
                                  list = FALSE, 
                                  times = 1)
#the distribution of dependent variables
knitr::kable(Desc(scat$Species)[[1]][["freq"]]) 
```

```{r}
#splitting

scatTrain <- scat[ trainIndex,]
scatTest  <- scat[-trainIndex,]

knitr::kable(Desc(scatTrain$Species)[[1]][["freq"]])

```

### Based on Predictors
If we manage to get a unsupervised learning prediction, the maxDissim function is a splitting good way using a maximum dissimilarity approach. About the method:

>Suppose there is a data set A with m samples and a larger data set B with n samples. We may want to create a subâ€“sample from B that is diverse when compared to A. To do this, for each sample in B, the function calculates the m dissimilarities between each point in A. The most dissimilar point in B is added to A and the process continues.

In a short, sampling out A as the start set, and getting the final set against start set by largest distance.

```{r echo=TRUE, warning=FALSE}

data(cars)

testing <- scale(cars[, c("Price", "Mileage")])
set.seed(5)
## A random sample of 5 data points
startSet <- sample(1:dim(testing)[1], 5)
samplePool <- testing[-startSet,]
start <- testing[startSet,]
newSamp <- maxDissim(start, samplePool, n = 20)
```

result of the splitting:

```{r}
plot(testing[-newSamp,], 
          col = "darkgrey")
points(start, pch = 16, cex = .7)
     
     for(i in seq(along = newSamp))
          points(
               samplePool[newSamp[i],1], 
               samplePool[newSamp[i],2], 
               pch = paste(i), col = "darkred")
```

More maxDissim methods, please check this [article](http://rismyhammer.com/ml/Pre-Processing.html)'s Splitting Based on Predictors part.


### Time Serias Data Splitting

`createTimeSlices()` is the sister function of `createDataPartition()` to handle time serias data. Three argument of the function is:

- `initialWindow`: the initial number of consecutive values in each training set sample
- `horizon`: The number of consecutive values in test set sample
- `fixedWindow`: A logical: if FALSE, the training set always start at the first sample and the training set size will vary over data splits.

The example of the function usage is also in this [articles](http://rismyhammer.com/ml/Pre-Processing.html)'s Time Series Data Splitting part.

```{r}
data(economics)
# which is often used in the `trainControl()` set
myTimeControl <- trainControl(method = "timeslice", initialWindow = 36, horizon = 12, fixedWindow = TRUE)

plsFitTime <- train(unemploy ~ pce + pop + psavert, data = economics, method = "pls",
                    preProc = c("center", "scale"), trControl = myTimeControl)

```


## Preprocessing

This part is based on the conclution of your data exploration. You need to consider what situation of your data that you need to deal with before starting build your model. According to the situation of the function facing to, preprocessing function of caret can be sorted as:


situation category|function
----------|--------
categorical/dichotomous variables| `dummyVars()`,`data.table()`
unbalance| `preProcess(df,method = c("center", "scale"))`,`nearZeroVar()`
missing value | `preProcess(df,method = c("center", "scale")/"knnImpute")`
too much predictors| `preProcess(df, method = "BoxCox"/"pca")`
class distance | `classDist()`

As you can see, there are a lot of situations that might causes model prediction fail, and there are more solution to fix the situations you face. No matter how unique you are getting the data set problems, **stable, numerical, centralized and no missing** data set may lead a better result of model. 

### `preProcess()` Function Set

Package caret provised a useful function,`preProcess()`,to handle the preprocessing's situations. 

Function preProcess contain two parts: `preProcess()`procession and `predict.preProcess()`procession. `preProcess()`estimates the required parameters for each operation and `predict.preProcess()` is used to apply them to specific data sets. This function can also be interfaces when calling the train function.

`preProcValues <- preProcess(dataToHandle, method = c(),...other Arguments)`
`dataTransformed <- predict(preProcValues, dataToHandle)`

Specific example:

```{r}
data(oil)

inTrain <- sample(seq(along = fattyAcids), length(fattyAcids)*0.7)

training <- fattyAcids[inTrain,]
str(training)

preProcValues <- preProcess(training, method = c("center", "scale"))

trainTransformed <- predict(preProcValues, training)
str(trainTransformed)

```


The list of function preProcess methods:

- `center`,`scale`, and `range` to normalize predictors.
- `BoxCox`, `YeoJohnson`, or `expoTrans` to transform predictors. 
- `knnImpute`, `bagImpute`, or `medianImpute` to impute. 
- `corr`, `nzv`, `zv`, and `conditionalX` to filter. 
- `pca`, `ica`, or `spatialSign` to transform groups.


More information about how function preProcess is used, you can find in `??preProcess` and this [article](http://rismyhammer.com/ml/Pre-Processing.html#split-data)


### Other preprocessing function

#### Turning Numerical 

There are two methods to deal with the categorical or dichotomous variables:


##### Recode the Categorical Variable as Numeric

```{r}

data("Zoo")
Zoo$animalNames <- rownames(Zoo)

#categorical variable
unique(Zoo$type)
## recode the categorical variable as numeric
value= unique(Zoo$type)
zoo <- setDT(Zoo)[,type:=as.character(type)][.(type = value,to = 1:length(value)),on = "type",type :=i.to]
unique(zoo$type)

```

##### `dummyVars()` in Caret

```{r}
#dichotomous variable
## dummyVars() in caret
head(model.matrix(type ~ ., data = zoo[,c(1:4,17)]),4)

dummies <- dummyVars(type ~ ., data = zoo[,c(1:4,17)])
head(predict(dummies, newdata = zoo),4)
```

##### `dummy_cols()` in FastDummies

About the function usage, you find more info in the [article](https://cran.r-project.org/web/packages/fastDummies/vignettes/making-dummy-variables.html).

```{r}
## dummyVars() in caret
data("Zoo")
Zoo$animalNames <- rownames(Zoo)

zooDummy <- dummy_cols(Zoo[,c(1:4,17)])
head(zooDummy,4)
```

#### Removing Correlated 

`findCorrelation()` is the tool to find the independences in model, that are be correlated with each other. Based on the result, you can remove or reduce the level of correlation between the independences.

check the correlated situation of the independences:

```{r}
data(Zoo)
descrCor <- cor(Zoo[,-17])
summary(descrCor[upper.tri(descrCor)])
```

set a cutoff line and find the independences above:

```{r}
CUTOFF <- 0.85 
cor_matrix <- cor(Zoo[,-17]) 
cor_high <- findCorrelation(cor_matrix, CUTOFF)
high_cor_remove <- row.names(cor_matrix)[cor_high] 
high_cor_remove
#removingCorZoo <- Zoo[,-high_cor_remove]
```

## Basic Model Training

### Quick example

```{r}

#data splitting
library(caret)
set.seed(127)
inTraining <- createDataPartition(Zoo$type, p = .75, list = FALSE)
training <- Zoo[ inTraining,]
testing  <- Zoo[-inTraining,]

#set train control
set.seed(127)
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)

metric <- "Accuracy"
set.seed(127)
mtry <- floor(sqrt(ncol(training)))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(type~., data=training, 
                    method="rf", metric=metric, 
                    tuneGrid=tunegrid, 
                    trControl=fitControl)
rf_default

```


### Backbone of the model

As you can see in the quick example, the `train()` is the center of the whole model training(About the origin of the function, [there](http://www.jstatsoft.org/article/view/v028i05/v28i05.pdf) is an illastration). 

The basic systax of the function `train()`

`train(y ~ x1 + x2, data = dat, ...)`
`train(x = predictor_df, y = outcome_vector, ...)` 
`train(recipe_object, data = dat, ...`

Except rfe, sbf, gafs, and safs only have the x/y interface, most basic part of the train consists:

- **x**: a matrix or data frame of predictors. Currently, the function only accepts numeric values (i.e., no factors or character variables). In some cases, the model.matrix function may be needed to generate a data frame or matrix of purely numeric data.

- **y**: a numeric or factor vector of outcomes. The function determines the type of problem (classification or regression) from the type of the response given in this argument.

- **method**: a character string specifying the type of model to be used.

`train(y ~ ., data = dat, method = "rf")`

### Subside of Model Train 

`trainControl()` and `expand.grid` are most often used option to specify the train model, and both of them can be an arguments in `train()` or as an independent parameters outside.

#### Resampling Options

To choose a resampling method:

`trainControl(method = <method>, <options>)`

Methods and options are:

- **"cv"** for K-fold cross-validation (`number` sets the # folds). 
- **"repeatedcv"** for repeated cross-validation (`repeats` for # repeats). 
- **"boot"** for bootstrap  (`number `sets the iterations). 
- **"LGOCV"** for leave-group-out (`number` and `p` are options). 
- **"LOO"** for leave-one-out cross-validation. 
- **"oob"** for out-of-bag resampling (only for some models). 
- **"timeslice"** for time-series data (options are initialWindow, horizon, fixedWindow, and skip).

#### Performance Metrics
To choose how to summarize a model, argument `metric` in `trainControl()`can be used to determine which is the best settings.

The methods of metric:

- **Accuracy and Kappa**, default for binary and multi-class classification datasets;
- **RMSE and R^2**, by defaut for regression datasets;
- **ROC (AUC, Sensitivity and Specificity)**,only suitable for binary classification;
- **LogLoss** to evaluate binary classification.

The usage example of matrics is in this [article](https://machinelearningmastery.com/machine-learning-evaluation-metrics-in-r/).

More specific metrics, argument `summaryFunction` is another option:

`trainControl(summaryFunction = <R function>, classProbs = <logical>)`

summaryFunction options:

defaultSummary (for accuracy, RMSE, etc), twoClassSummary(for ROC curves), and prSummary (for information retrieval). For the last two functions, the option classProbs must be set to TRUE.


#### Grid Search
To let train determine the values of the tuning parameter(s), the `tuneLength` option controls how many values per tuning parameter to evaluate. `tuneLength` is an integer denoting the amount of granularity in the tuning parameter grid. By default, this argument is the number of levels for each tuning parameters that should be generated by train.
About the tuneLength usage, this [post](https://stackoverflow.com/questions/38859705/r-understanding-caret-traintunelength-and-svm-methods-from-kernlab) is also helpful.

Or use tuneGrid argument:

`grid <- expand.grid(alpha = c(0.1, 0.5, 0.9), lambda = c(0.001, 0.01))`

`train(x = x, y = y, method = "glmnet",tuneGrid = grid)`

#### Subsampling

`sampling`is an argument to fix the large class imbalance situation. More information about class imbalance situation, please check this [article](https://www.r-bloggers.com/handling-class-imbalance-with-r-and-caret-an-introduction/)

`trainControl(sampling = "down")`


## After Trainging
### Quick example
```{r}
#preperation of data
data(oil)
fattyAcids <- cbind(fattyAcids,oilType)
fattyAcids <- setDT(fattyAcids)[,oilType:= 
                                  plyr::mapvalues(oilType,c("A", "B","C","D", "E", "F", "G"),
                                         c("pumpkin","sunflower","peanut",
                                           "olive","soybean","rapeseed","corn") )]

#splitting and training of data
set.seed(127)
inTraining <- createDataPartition(fattyAcids$oilType, p = .75, list = FALSE)
training <- fattyAcids[ inTraining,]
testing  <- fattyAcids[-inTraining,]

bootControl <- trainControl(number = 100)
set.seed(127)
svmFit <- train(oilType~., training, 
                method = "svmRadial", 
                tuneLength = 5,
                preProc = c("center", "scale"),
                trControl = bootControl, 
                scaled = FALSE)
#model result
svmFit
#the most optimal model 
svmFit$finalModel


#variable importance
svmImp <- varImp(svmFit, scale = FALSE)
plot(svmImp)

#model result measure
svmType <- predict(svmFit, newdata = testing)
confusionMatrix(data = svmType, testing$oilType)

```

### Variable Importance

> - For most classification models, each predictor will have a separate variable importance for each class.
> - All measures of importance are scaled to have a maximum value of 100, unless the scale argument of varImp.train is set to FALSE

About the specific metrics of importance,please visit this [part](http://topepo.github.io/caret/variable-importance.html#model-specific-metrics).


### Measure Perfomance


About the specific methods of measuring,please visit this [part](http://topepo.github.io/caret/measuring-performance.html).


#### Regression

`postResample(pred = modelFit, obs = testing$y)`

Example:

```{r}

library(mlbench)
data(BostonHousing)

set.seed(280)
bh_index <- createDataPartition(BostonHousing$medv, p = .75, list = FALSE)
bh_tr <- BostonHousing[ bh_index, ]
bh_te <- BostonHousing[-bh_index, ]

set.seed(7279)
lm_fit <- train(medv ~ . + rm:lstat,
                data = bh_tr, 
                method = "lm")
bh_pred <- predict(lm_fit, bh_te)

lm_fit

postResample(pred = bh_pred, obs = bh_te$medv)

```



#### Predicted Classes

`confusionMatrix(testPred,testing$obs)`


#### Class Probabilities

`twoClassSummary(testing, lev = levels(testing$obs))`





